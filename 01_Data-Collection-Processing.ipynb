{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "interesting-parking",
   "metadata": {},
   "source": [
    "# Processing Data Download\n",
    "\n",
    "This Jupyter Notebook serves the purpose to transform the downloaded data set (a collection of `.jpg` pictures) into interpretable input for the Machine Learning algorithm. As this has to be done only once, we keep it out of the main Jupyter Notebook that implements the different methods and models on the training and testing data set.\n",
    "\n",
    "Four steps have to be performed in this Jupyter Notebook:\n",
    "+ Proper loading of all images from the subdirectories\n",
    "+ Turning colored pictures into grayscale pictures\n",
    "+ Translate `.jpg` picture into pixels with intensity per pixel\n",
    "+ Store translated pixel data so that main Jupyter Notebook can access the data\n",
    "\n",
    "\n",
    "### Loading data\n",
    "\n",
    "The data is located in the folder `/01_data/00_raw/Masked-Face-Net-Dataset`. The folder CMFD (Correctly Masked Face Dataset) contains all pictures of the correctly worn facemasks whereas the folder IMFD (Incorrectly Masked Face Dataset) contains all pictures where people wear the facemasks incorrectly. In the folders there are several subdirectories labeled with ascending numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "lesbian-integer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries and set-up Jupyter Notebook.\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Imports for dealing with images:\n",
    "import PIL #Pillow (install with \"pip install Pillow\")\n",
    "\n",
    "# to make this notebook's output stable across runs (safety measure)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set path to correct and incorrect data sets for keeping references short later\n",
    "ROOT_DATA = \"01_data/99_dummy_toy_data\"\n",
    "PATH_DATA_CORRECT = os.path.join(ROOT_DATA + \"/correct\")\n",
    "PATH_DATA_INCORRECT = os.path.join(ROOT_DATA + \"/incorrect\")\n",
    "\n",
    "# Where to save possible figures\n",
    "PROJECT_ROOT_DIR = \"02_figures\"\n",
    "CHAPTER_ID = \"01_data_preparation\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "brief-transfer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries/functions\n",
    "from os import listdir\n",
    "from PIL import Image as PIL_Image\n",
    "\n",
    "# Set pixel size (--> Large amount pixels increase data massively.) (Original #pixels: 1024x1024)\n",
    "target_pixel = 64\n",
    "\n",
    "# Initalize and Load Correct pics\n",
    "loaded_pics_correct = np.array([])\n",
    "\n",
    "i = 0 # helping running index to distinguish between hstack and vstack\n",
    "for filename in listdir(PATH_DATA_CORRECT):\n",
    "    # open picture\n",
    "    pic = PIL_Image.open(PATH_DATA_CORRECT + \"/\" + filename)\n",
    "    # Reduce size from original format to target format\n",
    "    pic_resized = pic.resize((target_pixel, target_pixel))\n",
    "    # Extract RGB data\n",
    "    pic_data = np.array(pic_resized)\n",
    "    # Include help array to transform 3D-array(e.g.: 1024, 1024, 3) into one long array\n",
    "    help_array = np.array([[]])\n",
    "    help_array = np.append(help_array, pic_data)\n",
    "    # Stack help array onto loaded pics array so that each line has an array of features (RGB values per pixel)\n",
    "    if i == 0:\n",
    "        loaded_pics_correct = np.hstack((loaded_pics_correct, help_array))\n",
    "    else:\n",
    "        loaded_pics_correct = np.vstack((loaded_pics_correct, help_array))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promising-remedy",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Place to work on</b>\n",
    "<p>\n",
    "Write a function that takes different target pixels and creates different pickel outputs in the end. (In order to perform different test.)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "temporal-stage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initalize and Load Incorrect pics\n",
    "loaded_pics_incorrect = np.array([])\n",
    "\n",
    "i = 0 # helping running index to distinguish between hstack and vstack\n",
    "for filename in listdir(PATH_DATA_INCORRECT):\n",
    "    # open picture\n",
    "    pic = PIL_Image.open(PATH_DATA_INCORRECT + \"/\" + filename)\n",
    "    # Reduce size from original format to target format\n",
    "    pic_resized = pic.resize((target_pixel, target_pixel))\n",
    "    # Extract RGB data\n",
    "    pic_data = np.array(pic_resized)\n",
    "    # Include help array to transform 3D-array(e.g.: 1024, 1024, 3) into one long array\n",
    "    help_array = np.array([[]])\n",
    "    help_array = np.append(help_array, pic_data)\n",
    "    # Stack help array onto loaded pics array so that each line has an array of features (RGB values per pixel)\n",
    "    if i == 0:\n",
    "        loaded_pics_incorrect = np.hstack((loaded_pics_incorrect, help_array))\n",
    "    else:\n",
    "        loaded_pics_incorrect = np.vstack((loaded_pics_incorrect, help_array))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabulous-apparatus",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Place to work on</b>\n",
    "<p>\n",
    "Have a look into v-stack and h-stack formula. Not so smart to have a nested if clause in a for-loop. Have a look if append or cocatenate can solve the problem easier. But it is working for now.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiple-christmas",
   "metadata": {},
   "source": [
    "######## Clean below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "positive-thanksgiving",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 12288)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_pics_correct.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "mental-usage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pic_resized.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "hourly-dictionary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12288,)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "help_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "veterinary-denial",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64, 3)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pic_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "conceptual-isaac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 12288)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_pics_correct[:1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "hearing-mozambique",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(loaded_pics_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "incorrect-moore",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(loaded_pics_incorrect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manual-pierre",
   "metadata": {},
   "source": [
    "######## Clean above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expanded-cycle",
   "metadata": {},
   "source": [
    "### Adding labels and combining into one data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "intensive-terrorist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct pictures\n",
    "label_item = 1\n",
    "# Get length of pictures included\n",
    "len(loaded_pics_correct)\n",
    "labels_correct = np.array([])\n",
    "for itr in range(len(loaded_pics_correct)):\n",
    "    labels_correct = np.append(labels_correct, label_item)\n",
    "\n",
    "# Transform to integers\n",
    "labels_correct = labels_correct.astype(np.uint8)\n",
    "\n",
    "# Incorrect pictures\n",
    "label_item = 0\n",
    "# Get length of pictures included\n",
    "len(loaded_pics_incorrect)\n",
    "labels_incorrect = np.array([])\n",
    "for itr in range(len(loaded_pics_incorrect)):\n",
    "    labels_incorrect = np.append(labels_incorrect, label_item)\n",
    "\n",
    "# Transform to integers\n",
    "labels_incorrect = labels_incorrect.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "respective-demographic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(labels_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "inner-citizen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining labels into one array\n",
    "labels = np.array([])\n",
    "labels = np.append(labels_correct, labels_incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "sound-tongue",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0, 0], dtype=uint8)"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "unusual-interstate",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = np.array([])\n",
    "cleaned_data = np.vstack((loaded_pics_correct, loaded_pics_incorrect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "suspected-nursery",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 12288)"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "streaming-portfolio",
   "metadata": {},
   "outputs": [],
   "source": [
    "pic_data = {}\n",
    "pic_data[\"rgb_data\"] = cleaned_data\n",
    "pic_data[\"labels\"] = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assured-pixel",
   "metadata": {},
   "source": [
    "### Storing Pic data and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "focal-detector",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file = open(os.path.join(ROOT_DATA + \"/cleaned/pic_data.pkl\"),\"wb\")\n",
    "pickle.dump(pic_data, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrapped-firewall",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Place to work on</b>\n",
    "<p>\n",
    "Find more efficient way of storing data. Currently six pictures of combined 1.8 MB are transformed to csv files of 151 MB. ~Factor 84 in disk space (--> 40.9 GB get to 3,431 GB of data) - When 1024X1024 pixels are used.\n",
    "    \n",
    "Proposed pixel combination to run at first: (16x16, 32x32, 64x64) --> Reduces 1.8 MB of the six pictures to only 37 to 590 kB at max (reduction of factor 48 to 3 --> overall data reduced to 0.85 GB or 13.6 GB) \n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
