{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face-Mask-Classification Project\n",
    "\n",
    "\n",
    "Authors:\n",
    "+ Tobias Palmowski\n",
    "+ Fabian Metz\n",
    "+ Thilo Sander\n",
    "\n",
    "Date of Midterm-Report: 29.03.2021 <br>\n",
    "Date of final submission: 26.04.2021\n",
    "\n",
    "\n",
    "### Introduction\n",
    "\n",
    "This Jupyter Notebook is the core of the Face-Mask-Classification Project performed in the class \"Machine Learning\" of the Hertie School in Berlin. There is one other Jupyter Notebook which deals with combining the different datasets into one large data set - a task only performed once and therefore outsourced to another file.\n",
    "\n",
    "The following code is losely based on the chapter \"Classification\" from the book \"Hands-on Machine Learning with Scikit-Learn, Keras, and Tensorflow\" by Aurélien Géron.\n",
    "\n",
    "We included several hints were we are working on in yellow warning boxes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### Preparation\n",
    "\n",
    "<br>\n",
    "This part loads the necessary libraries and packages as well as setting the Input and Output Directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T15:27:15.227738Z",
     "start_time": "2021-04-19T15:27:15.222640Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries and set-up Jupyter NoteMasked-Face-Net-Datasetbook.\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns # for plotting\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "import timeit # To keep track of calculation time\n",
    "import PIL #Python Image Library\n",
    "import pickle\n",
    "\n",
    "# to make this notebook's output stable across runs (safety measure)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T15:27:17.060064Z",
     "start_time": "2021-04-19T15:27:17.052051Z"
    }
   },
   "outputs": [],
   "source": [
    "# Switch between toy and full data\n",
    "full_data_switch_on = False #Set True for full data set and False for Dummy Data set (see comment below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Action required</b>\n",
    "<p>\n",
    "    \n",
    "You have to set the switch whether you want to use the full dataset (True) or the dummy toy dataset (False). We set aside 100 correct and 100 incorrect pictures into a dummy toy data set in order to test our code faster. For running the algorithm with the dummy toy data everything is included in the GitHub-Repository (in the folder \"01_data/99_dummy_toy_data\"). The corresponding .pkl-file that include the output of the first Jupyter Notebook is included in the repository as well (in the folder \"01_data/01_cleaned/\")\n",
    "    \n",
    "However, if you want to run the algorithm with the full data set, you have to download the corresponding files under the Dropbox-Link below. The raw data is placed in the Dropbox folder \"00_raw\" (Hint: It is ca. 40,5 GB) and you have to download it into the repository folder \"01_data/00_raw/\". The corresponding pickle files depending on the pixel resolution you choose (see code at bottom of Jupyter Notebook) are several GB big and are also available in the Dropbox-Folder under \"01_cleaned\". This file has to be placed in the corresponding repository folder \"01_data/01_cleaned/\" to make this code run .\n",
    "\n",
    "The reason why we cannot directly use the links here is that we do not have figured out yet how to loop through subfolders and files Dropbox online. GitHub does not allow us to upload such an amount of data.\n",
    "<br>\n",
    "Dropbox-Link: https://www.dropbox.com/sh/45vbkq1ihfnhqem/AAADdq6mJKaLsG1w7SDK-QV8a?dl=0    \n",
    "\n",
    "<br>\n",
    "<b>\n",
    "!!!  Be aware: Running this Jupyter Notebook with the full data set requires several hours of runtime depending on your hardware !!!\n",
    "\n",
    "</b>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T15:27:19.178358Z",
     "start_time": "2021-04-19T15:27:19.165900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Defining path to cleaned data folder\n",
    "PATH_CLEANED_DATA = \"01_data/01_cleaned\"\n",
    "\n",
    "# Where to save figures\n",
    "ROOT_FIGS = \"02_figures\"\n",
    "TOPIC_ID = \"02_baseline\"\n",
    "IMAGES_PATH = os.path.join(ROOT_FIGS, TOPIC_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "CM_IMAGES_PATH = os.path.join(IMAGES_PATH, \"Confusion_Matrices\")\n",
    "os.makedirs(CM_IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, SAVE_PATH=IMAGES_PATH, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(SAVE_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\">... Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "\n",
    "# Where to save general output (not figures)\n",
    "ROOT_OUTPUT = \"03_output\"\n",
    "TOPIC_ID = \"01_eval_scores\"\n",
    "OUTPUT_PATH_EVAL = os.path.join(ROOT_OUTPUT, TOPIC_ID)\n",
    "os.makedirs(OUTPUT_PATH_EVAL, exist_ok=True)\n",
    "\n",
    "TOPIC_ID2 = \"02_error_tables\"\n",
    "OUTPUT_PATH_ERROR_TABLES = os.path.join(ROOT_OUTPUT, TOPIC_ID2)\n",
    "os.makedirs(OUTPUT_PATH_ERROR_TABLES, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## Functions\n",
    "\n",
    "<br>\n",
    "This part defines all necessary functions used in the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test/Show available pixel resolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T15:27:21.074341Z",
     "start_time": "2021-04-19T15:27:21.048452Z"
    }
   },
   "outputs": [],
   "source": [
    "# Show available pixel resolutions\n",
    "def show_avail_pixels():    \n",
    "    # List all files in cleaned data folder\n",
    "    filenames = []\n",
    "    for root, dirs, files in os.walk(PATH_CLEANED_DATA):\n",
    "        for name in files:\n",
    "            filenames.append(os.path.join(root, name))\n",
    "    \n",
    "    # Separate filenames of dummy toy and full data set (and extacting available pixels)        \n",
    "    pixels_dummy_toy = []\n",
    "    pixels_full = []\n",
    "    for i in range(len(filenames)):\n",
    "        if \"dummy_toy\" in filenames[i]:\n",
    "            # Find pixel resolution in filenames\n",
    "            search_term = \"dummy_toy_\"\n",
    "            pos_start = filenames[i].find(search_term)\n",
    "            pos_end = filenames[i].find(\".\")\n",
    "            help_length = len(search_term) #To add on starting posting\n",
    "            pixel_string = filenames[i][pos_start+help_length:pos_end]\n",
    "            # Append pixel resolutions to list\n",
    "            pixels_dummy_toy.append(pixel_string)\n",
    "        elif \"full\" in filenames[i]:\n",
    "            # Find pixel resolution in filenames\n",
    "            search_term = \"full_\"\n",
    "            pos_start = filenames[i].find(search_term)\n",
    "            pos_end = filenames[i].find(\".\")\n",
    "            help_length = len(search_term) #To add on starting posting\n",
    "            pixel_string = filenames[i][pos_start+help_length:pos_end]\n",
    "            # Append pixel resolutions to list\n",
    "            pixels_full.append(pixel_string)\n",
    "        else:\n",
    "            raise ValueError(\"Check filenames! Neither 'dummy_toy' nor 'full' is in filename...\")\n",
    "    \n",
    "    # Transforming pixel resolution strings into integers and sorting\n",
    "    for itr in range(len(pixels_dummy_toy)):\n",
    "        pixels_dummy_toy[itr] = int(pixels_dummy_toy[itr])\n",
    "    pixels_dummy_toy.sort()\n",
    "    pixels_full = [int(i) for i in pixels_full]\n",
    "    pixels_full.sort()\n",
    "    \n",
    "    # Output depending on switch\n",
    "    if full_data_switch_on == True:\n",
    "        print(\"The following pixels are available for the full data set: \", pixels_full)\n",
    "        return pixels_full\n",
    "    elif full_data_switch_on == False:\n",
    "        print(\"The following pixels are available for the dummy toy data set: \", pixels_dummy_toy)\n",
    "        return pixels_dummy_toy\n",
    "    else:\n",
    "        raise ValueError(\"Full data switch not correctly defined: Binary value of True or False necessary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T15:27:23.821486Z",
     "start_time": "2021-04-19T15:27:23.814508Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define Check of pixel resolutions\n",
    "def check_pixels(pix_res):\n",
    "    avail_pixels = show_avail_pixels()\n",
    "    if pix_res in avail_pixels:\n",
    "        print(\"The specified pixel resolution {} is available!\".format(pix_res))\n",
    "    else:\n",
    "        raise ValueError(\"The specified pixel resolution {} is not available! See available pixel resolutions above error message.\".format(pix_res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "#### Load and split specified data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T15:27:25.577395Z",
     "start_time": "2021-04-19T15:27:25.555930Z"
    }
   },
   "outputs": [],
   "source": [
    "# Definition of function to load corresponding pixel resolution file\n",
    "def load_data(pix_res):\n",
    "    # Define datafile path\n",
    "    if full_data_switch_on == True:\n",
    "        DATAFILE_PATH = os.path.join(PATH_CLEANED_DATA + \"/pic_data_full_\" + str(pix_res) + \".pkl\")\n",
    "    elif full_data_switch_on == False:\n",
    "        DATAFILE_PATH = os.path.join(PATH_CLEANED_DATA + \"/pic_data_dummy_toy_\" + str(pix_res) + \".pkl\")\n",
    "    else:\n",
    "        raise ValueError(\"Full data switch not correctly defined: Binary value of True or False necessary\")\n",
    "    \n",
    "    # Load pickle file\n",
    "    pic_data = pickle.load(open(DATAFILE_PATH,\"rb\"))\n",
    "    print(\">... Pickle datafile successfully loaded: \", DATAFILE_PATH)\n",
    "    \n",
    "    # Copying dictionary data into separate data frames\n",
    "    rgb_data, labels, pic_ids = pic_data[\"rgb_data\"], pic_data[\"labels\"], pic_data[\"pic_ids\"]\n",
    "    print(\">... Pickle dictionary sucessfully separated into individuals arrays\")\n",
    "    \n",
    "    # Split into test and training data set\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    rgb_data_train, rgb_data_test, labels_train, labels_test, pic_ids_train, pic_ids_test = train_test_split(rgb_data, labels, pic_ids, test_size=0.10, random_state=42)\n",
    "    print(\">... Data sucessfully split into test and train data\")\n",
    "    \n",
    "    # redefining labels as True False\n",
    "    labels_train_tf = (labels_train == 1)\n",
    "    labels_test_tf = (labels_test == 1)\n",
    "    \n",
    "    return rgb_data_train, rgb_data_test, labels_train_tf, labels_test_tf, pic_ids_train, pic_ids_test\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "#### Train specified classifier on data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T15:27:27.531675Z",
     "start_time": "2021-04-19T15:27:27.509737Z"
    }
   },
   "outputs": [],
   "source": [
    "# Definition of function that trains classifier on specified data set\n",
    "def train_clasf(classifier_x, rgb_train, labels_train, pixel_res):\n",
    "    #Start timer\n",
    "    start_time = timeit.default_timer()\n",
    "    \n",
    "    # Set classifier name and print status\n",
    "    classifier_name = str(classifier_x) # set classifier names\n",
    "    classifier_name = classifier_name[:classifier_name.find(\"(\")] \n",
    "    print(\">... Starting\", classifier_name)\n",
    "    \n",
    "    #Train model\n",
    "    classifier_x.fit(rgb_train, labels_train)\n",
    "    \n",
    "    # Stop timer:\n",
    "    time_elapsed = timeit.default_timer() - start_time\n",
    "    \n",
    "    # Store run-time info\n",
    "    run_time_info = {}\n",
    "    run_time_info[str(pixel_res)] = round(time_elapsed, 2)\n",
    "    if full_data_switch_on == True:\n",
    "        file = open((\"03_output/01_eval_scores/run_time_train_full_\"+str(classifier_name)+\"_\"+str(pixel_res)+\".pkl\"),\"wb\")\n",
    "        pickle.dump(run_time_info, file)\n",
    "        file.close()\n",
    "        print(\"Successfully stored in: 03_output/01_eval_scores/run_time_train_full_\"+str(classifier_name)+\"_\"+str(pixel_res)+\".pkl\")\n",
    "    elif full_data_switch_on == False:\n",
    "        file = open((\"03_output/01_eval_scores/run_time_train_dummy_toy_\"+str(classifier_name)+\"_\"+str(pixel_res)+\".pkl\"),\"wb\")\n",
    "        pickle.dump(run_time_info, file)\n",
    "        file.close()\n",
    "        print(\"Successfully stored in: 03_output/01_eval_scores/run_time_train_dummy_toy_\"+str(classifier_name)+\"_\"+str(pixel_res)+\".pkl\")\n",
    "    else:\n",
    "        raise ValueError(\"Full data switch not correctly defined: Binary value of True or False necessary\")\n",
    "    \n",
    "    print(\">... Classifier {} sucessfully trained in {} seconds.\".format(classifier_name, round(time_elapsed,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>CHECK WITH ALL</b>\n",
    "<p>\n",
    "    \n",
    "Do we really need this function?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "#### Evaluate specified classifier on specified data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T15:56:07.560907Z",
     "start_time": "2021-04-19T15:56:07.529011Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Define Evaluation function\n",
    "def eval_clasf(classifier_x, rgb_data, labels, pic_ids, pix_res):\n",
    "    # Import metrics and validation methods\n",
    "    from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "    from sklearn.metrics import confusion_matrix, precision_score, recall_score, plot_confusion_matrix, precision_recall_curve\n",
    "    import matplotlib.pyplot as plt \n",
    "    \n",
    "    # Start timer\n",
    "    start_time = timeit.default_timer()\n",
    "    # Set classifier name and print status\n",
    "    classifier_name = str(classifier_x) # set classifier names\n",
    "    classifier_name = classifier_name[:classifier_name.find(\"(\")] \n",
    "    print(\">... Starting evaluation of\", classifier_name)\n",
    "    \n",
    "    # Evaluate classifier\n",
    "    print(\">... Starting cross validation of\", classifier_name)\n",
    "    pred=cross_val_predict(classifier_x, rgb_data, labels, cv = 3) # cross value prediction with 3 folds\n",
    "    \n",
    "    \n",
    "    # Store evaluation metrics\n",
    "    print(\">... Saving evaluation metric of\", classifier_name)\n",
    "    # Running cross validation score\n",
    "    cvs = cross_val_score(classifier_x, rgb_data, labels, cv=3, scoring=\"accuracy\").round(3)\n",
    "    # Initialize eval dictionary and store values\n",
    "    evaluation_scores = {}\n",
    "    evaluation_scores[\"Precision Score\"] = precision_score(labels, pred).round(3)\n",
    "    evaluation_scores[\"Recall Score\"] = recall_score(labels, pred).round(3)\n",
    "    evaluation_scores[\"Confusion matrix\"] = confusion_matrix(labels, pred)\n",
    "    evaluation_scores[\"Cross Validation Accuracy Scores\"] = cvs\n",
    "    evaluation_scores[\"Cross Validation Accuracy Score Mean\"] = cvs.mean().round(3)\n",
    "    evaluation_scores[\"Cross Validation Accuracy Score Std\"] = cvs.std().round(5)\n",
    "  \n",
    "    \n",
    "    # Data for Precision recall curve //RandomForestClassifier //DecisionTreeClassifier\n",
    "    print(\">... Getting precision, recalls and thresholds\")\n",
    "    if classifier_name ==  'SGDClassifier':\n",
    "        decision_scores = cross_val_predict(classifier_x, rgb_data, labels, cv = 3, method=\"decision_function\")\n",
    "        precisions, recalls, thresholds = precision_recall_curve(labels,decision_scores)\n",
    "        #storing precision reacall and treshold values in eval directory\n",
    "        evaluation_scores[\"Precisions\"] = precisions\n",
    "        evaluation_scores[\"Recalls\"] = recalls\n",
    "        evaluation_scores[\"Thresholds\"] = thresholds\n",
    "        print(\">...precision, recalls and thresholds saved\") \n",
    "    elif classifier_name == 'LinearSVC':\n",
    "        decision_scores = cross_val_predict(classifier_x, rgb_data, labels, cv = 3, method=\"decision_function\")\n",
    "        precisions, recalls, thresholds = precision_recall_curve(labels,decision_scores)\n",
    "        #storing precision reacall and treshold values in eval directory\n",
    "        evaluation_scores[\"Precisions\"] = precisions\n",
    "        evaluation_scores[\"Recalls\"] = recalls\n",
    "        evaluation_scores[\"Thresholds\"] = thresholds\n",
    "        print(\">...precision, recalls and thresholds saved\")\n",
    "    else:\n",
    "         print (\">... precision recall not applicable for this classifier\")\n",
    "    \n",
    "    # Stop timer:\n",
    "    time_elapsed = timeit.default_timer() - start_time\n",
    "    evaluation_scores[\"Evaluation run-time in seconds\"] = round(time_elapsed, 2)\n",
    "    \n",
    "    # Storing evaluations scores into pickle file to access them later\n",
    "    if full_data_switch_on == True: \n",
    "        EVAL_FILE_PATH = os.path.join(OUTPUT_PATH_EVAL + \"/evaluation_scores_\" + str(classifier_name) + \"_full_\" + str(pix_res) + \".pkl\")\n",
    "    elif full_data_switch_on == False:\n",
    "        EVAL_FILE_PATH = os.path.join(OUTPUT_PATH_EVAL + \"/evaluation_scores_\" + str(classifier_name) + \"_dummy_toy_\" + str(pix_res) + \".pkl\")\n",
    "    else:\n",
    "        raise ValueError(\"Full data switch not correctly defined: Binary value of True or False necessary\")\n",
    "    file = open(EVAL_FILE_PATH,\"wb\")\n",
    "    pickle.dump(evaluation_scores, file)\n",
    "    file.close()\n",
    "    print(\">... Evaluation scores successfully stored in: {}\".format(EVAL_FILE_PATH))\n",
    "    \n",
    "    \n",
    "    # Create output array with error-types and pic ids\n",
    "    # Combine pics ids, labels and predicted values\n",
    "    print(\">... Starting to create output array with error-types for more transparency for classifier {}.\".format(classifier_name))\n",
    "    output_dict = {}\n",
    "    output_array = np.c_[pic_ids, labels, pred]\n",
    "    \n",
    "    # Create error array with specific error type\n",
    "    err_type_arr = np.array([])\n",
    "    for i in range(len(output_array)):\n",
    "        if output_array[i,1] == \"True\" and output_array[i,2] == \"False\":\n",
    "            err_type_arr = np.append(err_type_arr, \"False negative\")\n",
    "        elif output_array[i,1] == \"False\" and output_array[i,2] == \"True\":\n",
    "            err_type_arr = np.append(err_type_arr, \"False positive\")\n",
    "        else:\n",
    "            err_type_arr = np.append(err_type_arr, \"No error\")\n",
    "    \n",
    "    # Combine error into and output array into pandas data frame\n",
    "    error_table_pd = pd.DataFrame(output_array)\n",
    "    error_table_pd.rename(columns = {0:'Filename picture ID', 1:\"Label\", 2:\"Predicted\"}, inplace = True)\n",
    "    error_table_pd[\"Error Type\"] = err_type_arr\n",
    "    \n",
    "    # Saving all in dictionary\n",
    "    output_dict[\"error_table\"] = error_table_pd\n",
    "    \n",
    "    # Saving dictionary of error table\n",
    "    if full_data_switch_on == True: \n",
    "        ERROR_TABLE_FILE_PATH = os.path.join(OUTPUT_PATH_ERROR_TABLES + \"/error_table_pd_\" + str(classifier_name) + \"_full_\" + str(pix_res) + \".pkl\")\n",
    "    elif full_data_switch_on == False:\n",
    "        ERROR_TABLE_FILE_PATH = os.path.join(OUTPUT_PATH_ERROR_TABLES + \"/error_table_pd_\" + str(classifier_name) + \"_dummy_toy_\" + str(pix_res) + \".pkl\")\n",
    "    else:\n",
    "        raise ValueError(\"Full data switch not correctly defined: Binary value of True or False necessary\")\n",
    "    file = open(ERROR_TABLE_FILE_PATH,\"wb\")\n",
    "    pickle.dump(output_dict, file)\n",
    "    file.close()\n",
    "    print(\">... Error table dictionary successfully stored in: {}\".format(ERROR_TABLE_FILE_PATH))\n",
    "    \n",
    "    \n",
    "    # Output message\n",
    "    print(\">... Evaluation of classifier {} sucessfully finished in {} seconds.\".format(classifier_name, round(time_elapsed,2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "#### Define showing output functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T15:27:33.996217Z",
     "start_time": "2021-04-19T15:27:33.952291Z"
    }
   },
   "outputs": [],
   "source": [
    "# Defining function that shows and stores all showable outputs\n",
    "def show_evals(pix_res, error_table=True, conf_matrix=True, eval_metrics=True):\n",
    "\n",
    "    \n",
    "    ###### Error Table ######\n",
    "    if error_table == True:\n",
    "        # List all files of error table folder\n",
    "        filenames_error_table = []\n",
    "        for root, dirs, files in os.walk(OUTPUT_PATH_ERROR_TABLES):\n",
    "            for name in files:\n",
    "                filenames_error_table.append(os.path.join(root, name))\n",
    "        \n",
    "        # Select all filenames corresponding to pixel size\n",
    "        filenames_error_table_pix = []\n",
    "        for i in range(len(filenames_error_table)):\n",
    "            if str(pix_res) in filenames_error_table[i]:\n",
    "                filenames_error_table_pix.append(filenames_error_table[i])\n",
    "        \n",
    "        # Display error table\n",
    "        for i in range(len(filenames_error_table_pix)):\n",
    "            # Extract classifier name from filename\n",
    "            if \"dummy_toy\" in filenames_error_table_pix[i]:\n",
    "                # Find pixel resolution in filenames\n",
    "                search_term = \"error_table_pd_\"\n",
    "                pos_start = filenames_error_table_pix[i].find(search_term)\n",
    "                pos_end = filenames_error_table_pix[i].find(\"_dummy_toy_\")\n",
    "                help_length = len(search_term) #To add on starting posting\n",
    "                classifier_name = filenames_error_table_pix[i][pos_start+help_length:pos_end]\n",
    "            elif \"full\" in filenames_error_table_pix[i]:\n",
    "                # Find pixel resolution in filenames\n",
    "                search_term = \"error_table_pd_\"\n",
    "                pos_start = filenames_error_table_pix[i].find(search_term)\n",
    "                pos_end = filenames_error_table_pix[i].find(\"_full_\")\n",
    "                help_length = len(search_term) #To add on starting posting\n",
    "                classifier_name = filenames_error_table_pix[i][pos_start+help_length:pos_end]\n",
    "            else:\n",
    "                raise ValueError(\"Check filenames! Neither 'dummy_toy' nor 'full' is in filename...\")\n",
    "            \n",
    "            # Load pickle-file and display pandas data frame filtered to error types\n",
    "            dict_error_table = pickle.load(open(filenames_error_table_pix[i],\"rb\"))\n",
    "            print(\">... Error table pickle file successfully loaded for {} and pixel resolution {}\".format(classifier_name, pix_res))\n",
    "            error_table_pd = dict_error_table[\"error_table\"]\n",
    "            filter_options = [\"False positive\", \"False negative\"]\n",
    "            print(\">... Print error table for pixel resolution {} of {}\\n\\n\".format(pix_res, classifier_name))\n",
    "            print(error_table_pd.loc[error_table_pd[\"Error Type\"].isin(filter_options)].sort_values(by=[\"Label\", \"Filename picture ID\"]))\n",
    "            print(\"\\n\\n\")\n",
    "    \n",
    "    elif error_table == False:\n",
    "        print(\"Error Table for pixel resolution {} not selected by user.\".format(pix_res))\n",
    "    else:\n",
    "        raise ValueError(\"Check parameter 'error_table': Has to be binary True or False\")\n",
    "\n",
    "        \n",
    "    ###### Confusion Matrix ######\n",
    "    if conf_matrix == True:\n",
    "        # List all files of eval dictionary folder\n",
    "        filenames_eval_dics = []\n",
    "        for root, dirs, files in os.walk(OUTPUT_PATH_EVAL):\n",
    "            for name in files:\n",
    "                filenames_eval_dics.append(os.path.join(root, name))\n",
    "        \n",
    "        # Select all filenames corresponding to pixel size\n",
    "        filenames_eval_dics_pix = []\n",
    "        for i in range(len(filenames_eval_dics)):\n",
    "            if str(pix_res) in filenames_eval_dics[i]:\n",
    "                filenames_eval_dics_pix.append(filenames_eval_dics[i])\n",
    "        \n",
    "        # Display confusion matrix\n",
    "        for i in range(len(filenames_eval_dics_pix)):\n",
    "            # Extract classifier name from filename\n",
    "            if \"dummy_toy\" in filenames_eval_dics_pix[i]:\n",
    "                # Find pixel resolution in filenames\n",
    "                search_term = \"evaluation_scores_\"\n",
    "                pos_start = filenames_eval_dics_pix[i].find(search_term)\n",
    "                pos_end = filenames_eval_dics_pix[i].find(\"_dummy_toy_\")\n",
    "                help_length = len(search_term) #To add on starting posting\n",
    "                classifier_name = filenames_eval_dics_pix[i][pos_start+help_length:pos_end]\n",
    "            elif \"full\" in filenames_eval_dics_pix[i]:\n",
    "                # Find pixel resolution in filenames\n",
    "                search_term = \"evaluation_scores_\"\n",
    "                pos_start = filenames_eval_dics_pix[i].find(search_term)\n",
    "                pos_end = filenames_eval_dics_pix[i].find(\"_full_\")\n",
    "                help_length = len(search_term) #To add on starting posting\n",
    "                classifier_name = filenames_eval_dics_pix[i][pos_start+help_length:pos_end]\n",
    "            else:\n",
    "                raise ValueError(\"Check filenames! Neither 'dummy_toy' nor 'full' is in filename...\")\n",
    "        \n",
    "            # Load pikle-file and display pandas data frame filtered to error types\n",
    "            dict_eval_metrics = pickle.load(open(filenames_eval_dics_pix[i],\"rb\"))\n",
    "            print(\">... Eval metrics pickle file successfully loaded for {} and pixel resolution {}\".format(classifier_name, pix_res))\n",
    "            # Extract confusion matrix data into data frame\n",
    "            cm_data = dict_eval_metrics[\"Confusion matrix\"]\n",
    "            plot_matrix = pd.DataFrame(cm_data, columns = [\"classified as no mask\", \"classified as mask\"],\n",
    "                                index = [\"no mask\", \"mask\"])\n",
    "            \n",
    "            # Actual plot with seaborn and saving figure with different file names depending on switch\n",
    "            print(\">... Displaying confusion matrix for {} and pixel resolution {}\".format(classifier_name, pix_res))\n",
    "            if \"dummy_toy\" in filenames_eval_dics_pix[i]:\n",
    "                plt.figure(figsize = (5,5))\n",
    "                colormap = sns.color_palette(\"Reds\")\n",
    "                ax = plt.axes()\n",
    "                sns.heatmap(plot_matrix, ax = ax, annot = True, fmt='d', annot_kws={\"size\": 20}, cmap=colormap, cbar=False)\n",
    "                ax.set_title(classifier_name, fontsize= 15)\n",
    "                plt.show()\n",
    "                save_fig(\"99_CM_Toy_Dummy_{}_{}\".format(classifier_name, pix_res), SAVE_PATH=CM_IMAGES_PATH)\n",
    "            elif \"full\" in filenames_eval_dics_pix[i]:\n",
    "                plt.figure(figsize = (5,5))\n",
    "                colormap = sns.color_palette(\"Reds\")\n",
    "                ax = plt.axes()\n",
    "                sns.heatmap(plot_matrix, ax = ax, annot = True, fmt='d', annot_kws={\"size\": 20}, cmap=colormap, cbar=False)\n",
    "                ax.set_title(classifier_name, fontsize= 15)\n",
    "                plt.show()\n",
    "                save_fig(\"Confusion_Matrix_{}_{}\".format(classifier_name, pix_res), SAVE_PATH=CM_IMAGES_PATH)\n",
    "            else:\n",
    "                raise ValueError(\"Check filenames! Neither 'dummy_toy' nor 'full' is in filename...\")\n",
    "                \n",
    "    elif conf_matrix == False:\n",
    "        print(\"Confusion matrix for pixel resolution {} not selected by user.\".format(pix_res))\n",
    "    else:\n",
    "        raise ValueError(\"Check parameter 'conf_matrix': Has to be binary True or False\")           \n",
    "    \n",
    "    \n",
    "   \n",
    "   \n",
    "\n",
    "    ###### Eval Metrics ######\n",
    "    if eval_metrics == True:\n",
    "        # List all files of eval dictionary folder\n",
    "        filenames_eval_dics = []\n",
    "        for root, dirs, files in os.walk(OUTPUT_PATH_EVAL):\n",
    "            for name in files:\n",
    "                filenames_eval_dics.append(os.path.join(root, name))\n",
    "        \n",
    "        # Select all filenames corresponding to pixel size\n",
    "        filenames_eval_dics_pix = []\n",
    "        for i in range(len(filenames_eval_dics)):\n",
    "            if str(pix_res) in filenames_eval_dics[i]:\n",
    "                filenames_eval_dics_pix.append(filenames_eval_dics[i])\n",
    "                \n",
    "    \n",
    "            \n",
    "        \n",
    "################################################## Further input necessary ###################################        \n",
    "        eval_metrics = {}\n",
    "        for i in range(len(filenames_eval_dics_pix)):\n",
    "            # Extract classifier name from filename\n",
    "            if \"dummy_toy\" in filenames_eval_dics_pix[i]:\n",
    "                # Find pixel resolution in filenames\n",
    "                search_term = \"evaluation_scores_\"\n",
    "                pos_start = filenames_eval_dics_pix[i].find(search_term)\n",
    "                pos_end = filenames_eval_dics_pix[i].find(\"_dummy_toy_\")\n",
    "                help_length = len(search_term) #To add on starting posting\n",
    "                classifier_name = filenames_eval_dics_pix[i][pos_start+help_length:pos_end]\n",
    "            elif \"full\" in filenames_eval_dics_pix[i]:\n",
    "                # Find pixel resolution in filenames\n",
    "                search_term = \"evaluation_scores_\"\n",
    "                pos_start = filenames_eval_dics_pix[i].find(search_term)\n",
    "                pos_end = filenames_eval_dics_pix[i].find(\"_full_\")\n",
    "                help_length = len(search_term) #To add on starting posting\n",
    "                classifier_name = filenames_eval_dics_pix[i][pos_start+help_length:pos_end]\n",
    "            else:\n",
    "                raise ValueError(\"Check filenames! Neither 'dummy_toy' nor 'full' is in filename...\")\n",
    "            \n",
    "            \n",
    "            # Load pikle-file and display pandas data frame filtered to error types\n",
    "            dict_eval_metrics = pickle.load(open(filenames_eval_dics_pix[i],\"rb\"))\n",
    "            print(\">... Eval metrics pickle file successfully loaded for {} and pixel resolution {}\".format(classifier_name, pix_res))\n",
    "            # Extract eval metrics into data frame\n",
    "            precision_score = dict_eval_metrics[\"Precision Score\"]\n",
    "            recall_score = dict_eval_metrics[\"Recall Score\"]\n",
    "            cv_acc_scores = dict_eval_metrics[\"Cross Validation Accuracy Scores\"]\n",
    "            cv_acc_scores_mean = dict_eval_metrics[\"Cross Validation Accuracy Score Mean\"]\n",
    "            cv_acc_scores_std = dict_eval_metrics[\"Cross Validation Accuracy Score Std\"]\n",
    "            eval_run_time = dict_eval_metrics[\"Evaluation run-time in seconds\"]\n",
    "            \n",
    "            #Storing in eval_metrics library\n",
    "            eval_metrics[classifier_name] = [precision_score,\n",
    "                                             recall_score,\n",
    "                                             cv_acc_scores,\n",
    "                                             cv_acc_scores_mean,\n",
    "                                             cv_acc_scores_std,\n",
    "                                             eval_run_time]\n",
    "        \n",
    "        # Output table of metrics\n",
    "        print(\">... Display eval metrics table for pixel resolution {}\\n\\n\".format(pix_res))\n",
    "        index = [\"precision score\", \"recall score\", \"cross validation scores\", \"cross validation mean\", \"cross validation std\", \"eval run time in seconds\"]\n",
    "        table_eval_metrics = pd.DataFrame(eval_metrics, index)\n",
    "        print(table_eval_metrics, \"\\n\\n\")  \n",
    "    \n",
    "    elif eval_metrics == False:\n",
    "        print(\"Eval metrics for pixel resolution {} not selected by user.\".format(pix_res))\n",
    "    else:\n",
    "        raise ValueError(\"Check parameter 'eval_metrics': Has to be binary True or False\")      \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T15:27:39.106856Z",
     "start_time": "2021-04-19T15:27:39.075941Z"
    },
    "code_folding": [],
    "heading_collapsed": true
   },
   "outputs": [],
   "source": [
    "# define function Precision Recall Plots\n",
    "def eval_PR(pix_res,classifier_x):\n",
    "\n",
    "\n",
    "    ###### Precision-Recall Plots #####     \n",
    "    # List all files of eval dictionary folder\n",
    "    filenames_eval_dics = []\n",
    "    for root, dirs, files in os.walk(OUTPUT_PATH_EVAL):\n",
    "        for name in files:\n",
    "            filenames_eval_dics.append(os.path.join(root, name))\n",
    "\n",
    "    #Select all filenames corresponding to pixel size and specified classifiers\n",
    "    filenames_eval_dics_pix = []\n",
    "    for i in range(len(filenames_eval_dics)):\n",
    "        if str(classifier_x)[:str(classifier_x).find(\"(\")] in filenames_eval_dics[i] and str(pix_res) in filenames_eval_dics[i]:\n",
    "            filenames_eval_dics_pix.append(filenames_eval_dics[i])\n",
    "    \n",
    "    print(filenames_eval_dics_pix)\n",
    "\n",
    "    eval_metrics = {}\n",
    "    for i in range(len(filenames_eval_dics_pix)):\n",
    "        # Extract classifier name from filename\n",
    "        if \"dummy_toy\" in filenames_eval_dics_pix[i]:\n",
    "            # Find pixel resolution in filenames\n",
    "            search_term = \"evaluation_scores_\"\n",
    "            pos_start = filenames_eval_dics_pix[i].find(search_term)\n",
    "            pos_end = filenames_eval_dics_pix[i].find(\"_dummy_toy_\")\n",
    "            help_length = len(search_term) #To add on starting posting\n",
    "            classifier_name = filenames_eval_dics_pix[i][pos_start+help_length:pos_end]\n",
    "        elif \"full\" in filenames_eval_dics_pix[i]:\n",
    "            # Find pixel resolution in filenames\n",
    "            search_term = \"evaluation_scores_\"\n",
    "            pos_start = filenames_eval_dics_pix[i].find(search_term)\n",
    "            pos_end = filenames_eval_dics_pix[i].find(\"_full_\")\n",
    "            help_length = len(search_term) #To add on starting posting\n",
    "            classifier_name = filenames_eval_dics_pix[i][pos_start+help_length:pos_end]\n",
    "        else:\n",
    "            raise ValueError(\"Check filenames! Neither 'dummy_toy' nor 'full' is in filename...\")\n",
    "\n",
    "        # Load pikle-file and display pandas data frame filtered to error types\n",
    "        dict_eval_metrics = pickle.load(open(filenames_eval_dics_pix[i],\"rb\"))\n",
    "        print(\">... Eval metrics pickle file successfully loaded for {} and pixel resolution {}\".format(classifier_name, pix_res))\n",
    "        # Extract eval metrics into data frame\n",
    "        precisions = dict_eval_metrics[\"Precisions\"]\n",
    "        recalls = dict_eval_metrics[\"Recalls\"]\n",
    "        thresholds = dict_eval_metrics[\"Thresholds\"]\n",
    "\n",
    "        # Plot and saving figure with different file names depending on switch\n",
    "        print(\">... Displaying Precision-Recall Plot for {} and pixel resolution {}\".format(classifier_name, pix_res))\n",
    "        if \"dummy_toy\" in filenames_eval_dics_pix[i]:\n",
    "            plt.figure(figsize=(8,4))\n",
    "            plt.plot(thresholds, precisions[:-1],\"b--\",label = \"Precision\", linewidth=2)\n",
    "            plt.plot(thresholds, recalls[:-1], \"g--\",label = \"Recall\", linewidth=2)\n",
    "            plt.xlabel(\"Threshold\", fontsize = 16)\n",
    "            plt.legend(loc=\"upper left\", fontsize = 16)\n",
    "            plt.ylim([0,1])\n",
    "            plt.show()\n",
    "            #save_fig(\"99_PRP_Toy_Dummy_{}_{}\".format(classifier_name, pix_res), SAVE_PATH=PRP_IMAGES_PATH)\n",
    "        elif \"full\" in filenames_eval_dics_pix[i]:\n",
    "            plt.figure(figsize=(8,4))\n",
    "            plt.plot(thresholds, precisions[:-1],\"b--\",label = \"Precision\", linewidth=2)\n",
    "            plt.plot(thresholds, recalls[:-1], \"g--\",label = \"Recall\", linewidth=2)\n",
    "            plt.xlabel(\"Threshold\", fontsize = 16)\n",
    "            plt.legend(loc=\"upper left\", fontsize = 16)\n",
    "            plt.ylim([0,1])\n",
    "            plt.show()\n",
    "            #save_fig(\"Confusion_Matrix_{}_{}\".format(classifier_name, pix_res), SAVE_PATH=PRP_IMAGES_PATH)\n",
    "        else:\n",
    "            raise ValueError(\"Check filenames! Neither 'dummy_toy' nor 'full' is in filename...\")\n",
    "\n",
    "\n",
    "      \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T15:26:40.040231Z",
     "start_time": "2021-04-19T15:26:40.004786Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['03_output\\\\01_eval_scores\\\\evaluation_scores_LinearSVC_dummy_toy_16.pkl', '03_output\\\\01_eval_scores\\\\run_time_train_dummy_toy_LinearSVC_16.pkl']\n",
      ">... Eval metrics pickle file successfully loaded for LinearSVC and pixel resolution 16\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Precisions'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-274-009ca9dee912>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0meval_PR\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclassifier_LinSVC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-273-c1b9d669af55>\u001b[0m in \u001b[0;36meval_PR\u001b[1;34m(pix_res, classifier_x)\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\">... Eval metrics pickle file successfully loaded for {} and pixel resolution {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassifier_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpix_res\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;31m# Extract eval metrics into data frame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[0mprecisions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict_eval_metrics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Precisions\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[0mrecalls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict_eval_metrics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Recalls\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mthresholds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict_eval_metrics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Thresholds\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Precisions'"
     ]
    }
   ],
   "source": [
    "eval_PR(16,classifier_LinSVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T15:27:44.461016Z",
     "start_time": "2021-04-19T15:27:44.453076Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Precision Score': 0.947,\n",
       " 'Recall Score': 0.989,\n",
       " 'Confusion matrix': array([[85,  5],\n",
       "        [ 1, 90]], dtype=int64),\n",
       " 'Cross Validation Accuracy Scores': array([0.967, 0.95 , 0.983]),\n",
       " 'Cross Validation Accuracy Score Mean': 0.967,\n",
       " 'Cross Validation Accuracy Score Std': 0.01347,\n",
       " 'Evaluation run-time in seconds': 1.67}"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_eval_metrics = pickle.load(open('03_output\\\\01_eval_scores\\\\evaluation_scores_LinearSVC_dummy_toy_16.pkl',\"rb\"))\n",
    "       # Extract eval metrics into data frame\n",
    "dict_eval_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T15:26:40.046189Z",
     "start_time": "2021-04-19T15:26:39.822Z"
    }
   },
   "outputs": [],
   "source": [
    "classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T15:04:55.755583Z",
     "start_time": "2021-04-19T15:04:55.720614Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>TO DO</b>\n",
    "<p>\n",
    "    \n",
    "+ Saving of plots doesn't work properly yet. It saves a figure, but with no content.\n",
    "+ Displaying tables could be cooler and especially saving them would be great\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>ATTENTION</b>\n",
    "<p>\n",
    "Furhter sources for plotting things for precision and recall curves etc\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Use plot sklearn.metrics below more heavily: https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics --->Plotting\n",
    " \n",
    " ---> Visualizations: https://scikit-learn.org/stable/visualizations.html#visualizations\n",
    " \n",
    " metrics.plot_confusion_matrix(estimator, X, …)\n",
    "\t\n",
    "\n",
    "Plot Confusion Matrix.\n",
    "\n",
    "metrics.plot_det_curve(estimator, X, y, *[, …])\n",
    "\t\n",
    "\n",
    "Plot detection error tradeoff (DET) curve.\n",
    "\n",
    "metrics.plot_precision_recall_curve(…[, …])\n",
    "\t\n",
    "\n",
    "Plot Precision Recall Curve for binary classifiers.\n",
    "\n",
    "metrics.plot_roc_curve(estimator, X, y, *[, …])\n",
    "\t\n",
    "\n",
    "Plot Receiver operating characteristic (ROC) curve.\n",
    "\n",
    "\n",
    "metrics.ConfusionMatrixDisplay(…[, …])\n",
    "\t\n",
    "\n",
    "Confusion Matrix visualization.\n",
    "\n",
    "metrics.DetCurveDisplay(*, fpr, fnr[, …])\n",
    "\t\n",
    "\n",
    "DET curve visualization.\n",
    "\n",
    "metrics.PrecisionRecallDisplay(precision, …)\n",
    "\t\n",
    "\n",
    "Precision Recall visualization.\n",
    "\n",
    "metrics.RocCurveDisplay(*, fpr, tpr[, …])\n",
    "\t\n",
    "\n",
    "ROC Curve visualization.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "#### Define pipeline functions for evaluation and showing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T15:27:52.403539Z",
     "start_time": "2021-04-19T15:27:52.385586Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Defining pipeline function for load, training, evaluating\n",
    "\n",
    "def load_train_eval(list_of_pixels, list_of_classifiers):\n",
    "    for pix in list_of_pixels:\n",
    "        print(\"\\n\\n\\n>... Starting load_train_eval process for pixel resolution: {}.\".format(pix))\n",
    "        # Start timer pixel resolution\n",
    "        start_time_pix = timeit.default_timer()\n",
    "        # Test pixels\n",
    "        # check_pixels(pix)\n",
    "        # Load\n",
    "        rgb_data_train, rgb_data_test, labels_train, labels_test, pic_ids_train, pic_ids_test = load_data(pix)\n",
    "\n",
    "        # Run the different classifiers\n",
    "        for clasf in list_of_classifiers:\n",
    "            # Train\n",
    "            train_clasf(clasf, rgb_data_train, labels_train, pix)\n",
    "            # Evaluate\n",
    "            eval_clasf(clasf, rgb_data_train, labels_train, pic_ids_train, pix)\n",
    "\n",
    "        #End timer and output message\n",
    "        elapsed_pix = timeit.default_timer() - start_time_pix\n",
    "        print(\"\\n>... Ending load_train_eval process for pixel resolution: {}. Run-time: {} seconds.\".format(pix, round(elapsed_pix,2)))\n",
    "\n",
    "# Defining pipeline function for showing evaluations\n",
    "def show_evals_pipeline(list_of_pixels, error_table_switch=False, conf_matrix_switch=False, eval_metrics_switch=False):\n",
    "    for pix in list_of_pixels:\n",
    "        print(\"\\n\\n\\n>... Start showing evals for pixel resolution: {}.\".format(pix))\n",
    "        # Start timer pixel resolution\n",
    "        start_time_pix = timeit.default_timer()\n",
    "        \n",
    "        # Show evals\n",
    "        show_evals(pix, error_table=error_table_switch, conf_matrix=conf_matrix_switch, eval_metrics=eval_metrics_switch)\n",
    "        \n",
    "        #End timer and output message\n",
    "        elapsed_pix = timeit.default_timer() - start_time_pix\n",
    "        print(\"\\n>... Ending showing evals for pixel resolution: {}. Run-time: {} seconds.\".format(pix, round(elapsed_pix,2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "### Calling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T15:26:40.049177Z",
     "start_time": "2021-04-19T15:26:39.830Z"
    }
   },
   "outputs": [],
   "source": [
    "# Show available pixel resolutions\n",
    "# show_avail_pixels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Action required</b>\n",
    "<p>\n",
    "    \n",
    "Here you have to specify the pixel resolution with which you want to run the evaluations. Also be aware of the switch between full and dummy toy data set in the beginning of the jupyter notebook!\n",
    "<br>\n",
    "If you want to run the algorithm with the full data set, you have to download the corresponding pickle files (several GB) from the Dropbox-Folder under \"01_cleaned\". This file has to be placed in the corresponding repository folder to make this code run \"01_data/01_cleaned/\".\n",
    "<br>\n",
    "Dropbox-Link: https://www.dropbox.com/sh/45vbkq1ihfnhqem/AAADdq6mJKaLsG1w7SDK-QV8a?dl=0    \n",
    "\n",
    "<br>\n",
    "<b>\n",
    "!!!  Be aware: Running the this Jupyter Notebook with the full data set requires several hours of runtime depending on your hardware !!!\n",
    "\n",
    "</b>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T15:56:47.096987Z",
     "start_time": "2021-04-19T15:56:28.343737Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      ">... Starting load_train_eval process for pixel resolution: 16.\n",
      ">... Pickle datafile successfully loaded:  01_data/01_cleaned/pic_data_dummy_toy_16.pkl\n",
      ">... Pickle dictionary sucessfully separated into individuals arrays\n",
      ">... Data sucessfully split into test and train data\n",
      ">... Starting RandomForestClassifier\n",
      "Successfully stored in: 03_output/01_eval_scores/run_time_train_dummy_toy_RandomForestClassifier_16.pkl\n",
      ">... Classifier RandomForestClassifier sucessfully trained in 0.25 seconds.\n",
      ">... Starting evaluation of RandomForestClassifier\n",
      ">... Starting cross validation of RandomForestClassifier\n",
      ">... Saving evaluation metric of RandomForestClassifier\n",
      ">... Getting precision, recalls and thresholds\n",
      ">... precision recall not applicable for this classifier\n",
      ">... Evaluation scores successfully stored in: 03_output\\01_eval_scores/evaluation_scores_RandomForestClassifier_dummy_toy_16.pkl\n",
      ">... Starting to create output array with error-types for more transparency for classifier RandomForestClassifier.\n",
      ">... Error table dictionary successfully stored in: 03_output\\02_error_tables/error_table_pd_RandomForestClassifier_dummy_toy_16.pkl\n",
      ">... Evaluation of classifier RandomForestClassifier sucessfully finished in 1.09 seconds.\n",
      ">... Starting DecisionTreeClassifier\n",
      "Successfully stored in: 03_output/01_eval_scores/run_time_train_dummy_toy_DecisionTreeClassifier_16.pkl\n",
      ">... Classifier DecisionTreeClassifier sucessfully trained in 0.04 seconds.\n",
      ">... Starting evaluation of DecisionTreeClassifier\n",
      ">... Starting cross validation of DecisionTreeClassifier\n",
      ">... Saving evaluation metric of DecisionTreeClassifier\n",
      ">... Getting precision, recalls and thresholds\n",
      ">... precision recall not applicable for this classifier\n",
      ">... Evaluation scores successfully stored in: 03_output\\01_eval_scores/evaluation_scores_DecisionTreeClassifier_dummy_toy_16.pkl\n",
      ">... Starting to create output array with error-types for more transparency for classifier DecisionTreeClassifier.\n",
      ">... Error table dictionary successfully stored in: 03_output\\02_error_tables/error_table_pd_DecisionTreeClassifier_dummy_toy_16.pkl\n",
      ">... Evaluation of classifier DecisionTreeClassifier sucessfully finished in 0.09 seconds.\n",
      ">... Starting LinearSVC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabia\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully stored in: 03_output/01_eval_scores/run_time_train_dummy_toy_LinearSVC_16.pkl\n",
      ">... Classifier LinearSVC sucessfully trained in 0.31 seconds.\n",
      ">... Starting evaluation of LinearSVC\n",
      ">... Starting cross validation of LinearSVC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabia\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\fabia\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">... Saving evaluation metric of LinearSVC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabia\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\fabia\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">... Getting precision, recalls and thresholds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabia\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\fabia\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">...precision, recalls and thresholds saved\n",
      ">... Evaluation scores successfully stored in: 03_output\\01_eval_scores/evaluation_scores_LinearSVC_dummy_toy_16.pkl\n",
      ">... Starting to create output array with error-types for more transparency for classifier LinearSVC.\n",
      ">... Error table dictionary successfully stored in: 03_output\\02_error_tables/error_table_pd_LinearSVC_dummy_toy_16.pkl\n",
      ">... Evaluation of classifier LinearSVC sucessfully finished in 2.48 seconds.\n",
      ">... Starting SGDClassifier\n",
      "Successfully stored in: 03_output/01_eval_scores/run_time_train_dummy_toy_SGDClassifier_16.pkl\n",
      ">... Classifier SGDClassifier sucessfully trained in 0.01 seconds.\n",
      ">... Starting evaluation of SGDClassifier\n",
      ">... Starting cross validation of SGDClassifier\n",
      ">... Saving evaluation metric of SGDClassifier\n",
      ">... Getting precision, recalls and thresholds\n",
      ">...precision, recalls and thresholds saved\n",
      ">... Evaluation scores successfully stored in: 03_output\\01_eval_scores/evaluation_scores_SGDClassifier_dummy_toy_16.pkl\n",
      ">... Starting to create output array with error-types for more transparency for classifier SGDClassifier.\n",
      ">... Error table dictionary successfully stored in: 03_output\\02_error_tables/error_table_pd_SGDClassifier_dummy_toy_16.pkl\n",
      ">... Evaluation of classifier SGDClassifier sucessfully finished in 0.05 seconds.\n",
      "\n",
      ">... Ending load_train_eval process for pixel resolution: 16. Run-time: 4.36 seconds.\n",
      "\n",
      "\n",
      "\n",
      ">... Starting load_train_eval process for pixel resolution: 32.\n",
      ">... Pickle datafile successfully loaded:  01_data/01_cleaned/pic_data_dummy_toy_32.pkl\n",
      ">... Pickle dictionary sucessfully separated into individuals arrays\n",
      ">... Data sucessfully split into test and train data\n",
      ">... Starting RandomForestClassifier\n",
      "Successfully stored in: 03_output/01_eval_scores/run_time_train_dummy_toy_RandomForestClassifier_32.pkl\n",
      ">... Classifier RandomForestClassifier sucessfully trained in 0.26 seconds.\n",
      ">... Starting evaluation of RandomForestClassifier\n",
      ">... Starting cross validation of RandomForestClassifier\n",
      ">... Saving evaluation metric of RandomForestClassifier\n",
      ">... Getting precision, recalls and thresholds\n",
      ">... precision recall not applicable for this classifier\n",
      ">... Evaluation scores successfully stored in: 03_output\\01_eval_scores/evaluation_scores_RandomForestClassifier_dummy_toy_32.pkl\n",
      ">... Starting to create output array with error-types for more transparency for classifier RandomForestClassifier.\n",
      ">... Error table dictionary successfully stored in: 03_output\\02_error_tables/error_table_pd_RandomForestClassifier_dummy_toy_32.pkl\n",
      ">... Evaluation of classifier RandomForestClassifier sucessfully finished in 1.14 seconds.\n",
      ">... Starting DecisionTreeClassifier\n",
      "Successfully stored in: 03_output/01_eval_scores/run_time_train_dummy_toy_DecisionTreeClassifier_32.pkl\n",
      ">... Classifier DecisionTreeClassifier sucessfully trained in 0.1 seconds.\n",
      ">... Starting evaluation of DecisionTreeClassifier\n",
      ">... Starting cross validation of DecisionTreeClassifier\n",
      ">... Saving evaluation metric of DecisionTreeClassifier\n",
      ">... Getting precision, recalls and thresholds\n",
      ">... precision recall not applicable for this classifier\n",
      ">... Evaluation scores successfully stored in: 03_output\\01_eval_scores/evaluation_scores_DecisionTreeClassifier_dummy_toy_32.pkl\n",
      ">... Starting to create output array with error-types for more transparency for classifier DecisionTreeClassifier.\n",
      ">... Error table dictionary successfully stored in: 03_output\\02_error_tables/error_table_pd_DecisionTreeClassifier_dummy_toy_32.pkl\n",
      ">... Evaluation of classifier DecisionTreeClassifier sucessfully finished in 0.33 seconds.\n",
      ">... Starting LinearSVC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabia\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully stored in: 03_output/01_eval_scores/run_time_train_dummy_toy_LinearSVC_32.pkl\n",
      ">... Classifier LinearSVC sucessfully trained in 1.56 seconds.\n",
      ">... Starting evaluation of LinearSVC\n",
      ">... Starting cross validation of LinearSVC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabia\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">... Saving evaluation metric of LinearSVC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabia\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">... Getting precision, recalls and thresholds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabia\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">...precision, recalls and thresholds saved\n",
      ">... Evaluation scores successfully stored in: 03_output\\01_eval_scores/evaluation_scores_LinearSVC_dummy_toy_32.pkl\n",
      ">... Starting to create output array with error-types for more transparency for classifier LinearSVC.\n",
      ">... Error table dictionary successfully stored in: 03_output\\02_error_tables/error_table_pd_LinearSVC_dummy_toy_32.pkl\n",
      ">... Evaluation of classifier LinearSVC sucessfully finished in 10.76 seconds.\n",
      ">... Starting SGDClassifier\n",
      "Successfully stored in: 03_output/01_eval_scores/run_time_train_dummy_toy_SGDClassifier_32.pkl\n",
      ">... Classifier SGDClassifier sucessfully trained in 0.03 seconds.\n",
      ">... Starting evaluation of SGDClassifier\n",
      ">... Starting cross validation of SGDClassifier\n",
      ">... Saving evaluation metric of SGDClassifier\n",
      ">... Getting precision, recalls and thresholds\n",
      ">...precision, recalls and thresholds saved\n",
      ">... Evaluation scores successfully stored in: 03_output\\01_eval_scores/evaluation_scores_SGDClassifier_dummy_toy_32.pkl\n",
      ">... Starting to create output array with error-types for more transparency for classifier SGDClassifier.\n",
      ">... Error table dictionary successfully stored in: 03_output\\02_error_tables/error_table_pd_SGDClassifier_dummy_toy_32.pkl\n",
      ">... Evaluation of classifier SGDClassifier sucessfully finished in 0.16 seconds.\n",
      "\n",
      ">... Ending load_train_eval process for pixel resolution: 32. Run-time: 14.38 seconds.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ##### BREAK ##### \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ">... Start showing evals for pixel resolution: 16.\n",
      "Error Table for pixel resolution 16 not selected by user.\n",
      "Confusion matrix for pixel resolution 16 not selected by user.\n",
      "Eval metrics for pixel resolution 16 not selected by user.\n",
      "\n",
      ">... Ending showing evals for pixel resolution: 16. Run-time: 0.0 seconds.\n",
      "\n",
      "\n",
      "\n",
      ">... Start showing evals for pixel resolution: 32.\n",
      "Error Table for pixel resolution 32 not selected by user.\n",
      "Confusion matrix for pixel resolution 32 not selected by user.\n",
      "Eval metrics for pixel resolution 32 not selected by user.\n",
      "\n",
      ">... Ending showing evals for pixel resolution: 32. Run-time: 0.0 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Also be aware of switch between full and toy data set in the beginning of the jupyter notebook!\n",
    "\n",
    "\n",
    "\n",
    "####### Pixels #######\n",
    "# Specify pixel resolutions\n",
    "pixels = [16, 32] #, 48, 64\n",
    "\n",
    "\n",
    "\n",
    "####### Classifiers #######\n",
    "# import and define classifiers \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm, tree\n",
    "# from sklearn.neighbors import KNeighborsClassifier # KNN had several hours runtime and kernel always died before finising\n",
    "from sklearn.linear_model import SGDClassifier#, LogisticRegression # LogReg hat several hours runtime and kernel always died before finishing\n",
    "classifier_RandomForest = RandomForestClassifier(random_state=42)\n",
    "classifier_DecTree = tree.DecisionTreeClassifier(random_state=42)\n",
    "classifier_LinSVC = svm.LinearSVC(max_iter=4000, tol=1e-3, random_state=42) #linear as normal (c based) is impractical using large datasets\n",
    "classifier_SGD = SGDClassifier(max_iter=1000, tol=1e-3, random_state=42)\n",
    "# classifier_KNN = KNeighborsClassifier(n_neighbors=1)\n",
    "# classifier_LogReg = LogisticRegression(max_iter=2000, tol=1e-3,random_state=42)\n",
    "\n",
    "\n",
    "# Combine classifiers to a list through which you can loop\n",
    "classifiers= [ classifier_RandomForest, classifier_DecTree,classifier_LinSVC, classifier_SGD]\n",
    "\n",
    "\n",
    "\n",
    "####### Actual running #######\n",
    "load_train_eval(pixels, classifiers)\n",
    "print(\"\\n\\n\\n\\n\\n\\n\\n\\n\",\"##### BREAK #####\",\"\\n\\n\\n\\n\\n\\n\\n\\n\")\n",
    "show_evals_pipeline(pixels, error_table_switch=False, conf_matrix_switch=False, eval_metrics_switch=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
